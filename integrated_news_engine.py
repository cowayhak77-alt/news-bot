import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
import random
from datetime import datetime
# ìˆ˜ì§‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì¶”ê°€/ìˆ˜ì • ê°€ëŠ¥)
# ìˆ˜ì§‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì • (íŠ¹ë³„ì‹œ, ê´‘ì—­ì‹œ, ë„ì²­, ì„œìš¸ êµ¬ì²­ í¬í•¨)
TARGET_SITES = [
    # íŠ¹ë³„ì‹œ ë° ê´‘ì—­ì‹œ
    {"name": "ì„œìš¸íŠ¹ë³„ì‹œì²­", "url": "https://www.seoul.go.kr/news/news_report.do"},
    {"name": "ë¶€ì‚°ê´‘ì—­ì‹œì²­", "url": "https://www.busan.go.kr/nbgosi"},
    {"name": "ëŒ€êµ¬ê´‘ì—­ì‹œì²­", "url": "https://www.daegu.go.kr/index.do?menu_id=00000052"},
    {"name": "ì¸ì²œê´‘ì—­ì‹œì²­", "url": "https://www.incheon.go.kr/ic010205"},
    {"name": "ê´‘ì£¼ê´‘ì—­ì‹œì²­", "url": "https://www.gwangju.go.kr/boardList.do?boardId=BD_0000000027&menuId=gwangju0303010000"},
    {"name": "ëŒ€ì „ê´‘ì—­ì‹œì²­", "url": "https://www.daejeon.go.kr/drh/drhBoardList.do?boardId=normal_0007&menuSeq=1631"},
    {"name": "ìš¸ì‚°ê´‘ì—­ì‹œì²­", "url": "https://www.ulsan.go.kr/u/rep/bbs/list.ulsan?bbsId=BBS_0000000000000027&mId=001004003001000000"},
    {"name": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œì²­", "url": "https://www.sejong.go.kr/bbs/R0071/list.do"},

    # ë„ì²­ ë‹¨ìœ„
    {"name": "ê²½ê¸°ë„ì²­", "url": "https://www.gg.go.kr/bbs/board.do?bsIdx=469&menuId=1535"},
    {"name": "ê°•ì›íŠ¹ë³„ìì¹˜ë„ì²­", "url": "https://www.provin.gangwon.kr/gw/portal/sub03_01_01"},
    {"name": "ì¶©ì²­ë¶ë„ì²­", "url": "https://www.chungbuk.go.kr/www/selectBbsNttList.do?bbsNo=3271&key=1552"},
    {"name": "ì¶©ì²­ë‚¨ë„ì²­", "url": "https://www.chungnam.go.kr/cnportal/cnapcPressList/cnapcPress/list.do?menuNo=500498"},
    {"name": "ì „ë¶íŠ¹ë³„ìì¹˜ë„ì²­", "url": "https://www.jeonbuk.go.kr/board/list.jeonbuk?boardId=BODO_DATA&menuId=DOM_000000102001001000"},
    {"name": "ì „ë¼ë‚¨ë„ì²­", "url": "https://www.jeonnam.go.kr/M7124/boardList.do?menuId=jeonnam0201000000"},
    {"name": "ê²½ìƒë¶ë„ì²­", "url": "https://www.gb.go.kr/Main/page.do?mnu_uid=6792"},
    {"name": "ê²½ìƒë‚¨ë„ì²­", "url": "https://www.gyeongnam.go.kr/board/list.gyeongnam?boardId=BBS_0000057&menuId=DOM_000000102001001000"},
    {"name": "ì œì£¼íŠ¹ë³„ìì¹˜ë„ì²­", "url": "https://www.jeju.go.kr/news/bodo.htm"},

    # ì„œìš¸ì‹œ 25ê°œ êµ¬ì²­
    {"name": "ì¢…ë¡œêµ¬ì²­", "url": "https://www.jongno.go.kr/portal/bbs/B0000002/list.do?menuNo=1754"},
    {"name": "ì¤‘êµ¬ì²­", "url": "https://www.junggu.seoul.kr/news/board/list.do?bbsId=BBSMSTR_000000000031&menuNo=200045"},
    {"name": "ìš©ì‚°êµ¬ì²­", "url": "https://www.yongsan.go.kr/portal/bbs/B0000002/list.do?menuNo=200190"},
    {"name": "ì„±ë™êµ¬ì²­", "url": "https://www.sd.go.kr/main/selectBbsNttList.do?bbsNo=183&key=1476"},
    {"name": "ê´‘ì§„êµ¬ì²­", "url": "https://www.gwangjin.go.kr/portal/bbs/B0000002/list.do?menuNo=200191"},
    {"name": "ë™ëŒ€ë¬¸êµ¬ì²­", "url": "https://www.ddm.go.kr/www/selectBbsNttList.do?bbsNo=41&key=69"},
    {"name": "ì¤‘ë‘êµ¬ì²­", "url": "https://www.jungnang.go.kr/portal/bbs/B0000002/list.do?menuNo=200461"},
    {"name": "ì„±ë¶êµ¬ì²­", "url": "https://www.sb.go.kr/main/selectBbsNttList.do?bbsNo=3&key=151"},
    {"name": "ê°•ë¶êµ¬ì²­", "url": "https://www.gangbuk.go.kr/portal/bbs/B0000002/list.do?menuNo=200192"},
    {"name": "ë„ë´‰êµ¬ì²­", "url": "https://www.dobong.go.kr/bbs.asp?code=10004132"},
    {"name": "ë…¸ì›êµ¬ì²­", "url": "https://www.nowon.kr/www/user/bbs/BD_selectBbsList.do?q_bbsCode=1001&q_menuSn=12"},
    {"name": "ì€í‰êµ¬ì²­", "url": "https://www.ep.go.kr/CmsWeb/viewPage.do?version=1&menuId=MN20210204000000002"},
    {"name": "ì„œëŒ€ë¬¸êµ¬ì²­", "url": "https://www.sdm.go.kr/news/news/report.do"},
    {"name": "ë§ˆí¬êµ¬ì²­", "url": "https://www.mapo.go.kr/site/main/board/news/list"},
    {"name": "ì–‘ì²œêµ¬ì²­", "url": "https://www.yangcheon.go.kr/site/main/board/news/list"},
    {"name": "ê°•ì„œêµ¬ì²­", "url": "https://www.gangseo.seoul.kr/news/news010101"},
    {"name": "êµ¬ë¡œêµ¬ì²­", "url": "https://www.guro.go.kr/www/selectBbsNttList.do?bbsNo=642&key=1787"},
    {"name": "ê¸ˆì²œêµ¬ì²­", "url": "https://www.geumcheon.go.kr/portal/selectBbsNttList.do?bbsNo=151&key=198"},
    {"name": "ì˜ë“±í¬êµ¬ì²­", "url": "https://www.ydp.go.kr/www/selectBbsNttList.do?bbsNo=40&key=2791"},
    {"name": "ë™ì‘êµ¬ì²­", "url": "https://www.dongjak.go.kr/portal/bbs/B0000002/list.do?menuNo=200635"},
    {"name": "ê´€ì•…êµ¬ì²­", "url": "https://www.gwanak.go.kr/site/gwanak/ex/bbs/List.do?cbIdx=239"},
    {"name": "ì„œì´ˆêµ¬ì²­", "url": "https://www.seocho.go.kr/site/seocho/ex/bbs/List.do?cbIdx=243"},
    {"name": "ê°•ë‚¨êµ¬ì²­", "url": "https://www.gangnam.go.kr/board/B_000001/list.do?menuNo=GS040101"},
    {"name": "ì†¡íŒŒêµ¬ì²­", "url": "https://www.songpa.go.kr/www/selectBbsNttList.do?bbsNo=7&key=2775"},
    {"name": "ê°•ë™êµ¬ì²­", "url": "https://www.gangdong.go.kr/web/newportal/press/list"}
]



class IntegratedNewsEngine:
    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        # ëˆì´ ë˜ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        self.money_keywords = ["ë³´ë„", "ìë£Œ", "ê³µê³ ", "ì§€ì›", "ì‚¬ì—…", "ëª¨ì§‘", "ì„ ì •", "ì˜ˆì‚°", "íˆ¬ì", "ìœ¡ì„±", "í˜œíƒ", "ë³´ì¡°ê¸ˆ"]
        self.table_patterns = [
            "table.board-list", "table.list_table", "table.bbs_list", 
            "table.tbl_board", "table.tstyle_list", ".board_list table",
            "table[summary*='ê²Œì‹œíŒ']", "table.table", ".board_list", ".list_type",
            ".news_list", ".bbsList", ".boardList", ".list_item"
        ]

    def get_headers(self):
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.google.com/",
            "Connection": "keep-alive"
        }

    def is_money_news(self, title):
        # í‚¤ì›Œë“œ í•„í„°ë§ ë¡œì§
        for kw in self.money_keywords:
            if kw in title:
                return True
        return False

    def smart_scrape(self, url):
        try:
            session = requests.Session()
            response = session.get(url, headers=self.get_headers(), timeout=15)
            response.raise_for_status()
            
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
                
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = []
            for pattern in self.table_patterns:
                areas = soup.select(pattern)
                for area in areas:
                    found = area.select("tbody tr, tr, li, .item, .list_item, .post-item")
                    if found: rows.extend(found)
            
            if not rows:
                content_area = soup.select_one("#contents, #content, .content, main")
                if content_area:
                    rows = content_area.select("tr, li, div[class*='item']")
                else:
                    rows = soup.select("tr, li")

            results = []
            seen_titles = set()
            
            for row in rows:
                links = row.find_all("a")
                if not links: continue
                
                valid_links = [l for l in links if len(l.get_text(strip=True)) > 5]
                if not valid_links: continue
                
                title_tag = max(valid_links, key=lambda x: len(x.get_text(strip=True)))
                title = title_tag.get_text(strip=True)
                title = re.sub(r"\[ê³µì§€\]|\[ìƒˆê¸€\]|NEW", "", title).strip()
                
                if title in seen_titles or len(title) < 5: continue
                
                # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
                if not self.is_money_news(title): continue
                
                seen_titles.add(title)
                link = urljoin(url, title_tag['href'])
                
                results.append({
                    "title": title,
                    "link": link,
                    "date": datetime.now().strftime("%Y-%m-%d") # ì‹¤ì œ ë‚ ì§œ ì¶”ì¶œ ë¡œì§ì€ ì´ì „ ì½”ë“œ ì°¸ê³ 
                })
                if len(results) >= 5: break # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 5ê±´ë§Œ ìˆ˜ì§‘
                
            return results
        except Exception as e:
            return None

    def run(self):
        collected_data = []
        total_count = 0
        
        for site in TARGET_SITES:
            print(f"[{site['name']}] ìˆ˜ì§‘ ì¤‘...")
            news_list = self.smart_scrape(site['url'])
            if news_list:
                collected_data.append({"site_name": site['name'], "news_list": news_list})
                total_count += len(news_list)
            time.sleep(random.uniform(0.1, 0.5))

        # 1. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
        report = []
        report.append(f"ğŸ“… ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ” í•„í„° í‚¤ì›Œë“œ: {', '.join(self.money_keywords)}")
        report.append("="*60)
        
        for entry in collected_data:
            report.append(f"\nğŸ“Œ {entry['site_name']} ({len(entry['news_list'])}ê±´)")
            for news in entry['news_list']:
                report.append(f"- {news['title']}")
                report.append(f"  ğŸ”— {news['link']}")
        
        report.append("\n" + "="*60)
        report.append(f"âœ… ì´ {total_count}ê±´ì˜ 'ëˆ ë˜ëŠ” ì •ë³´'ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        final_report_txt = "\n".join(report)
        
        with open("daily_news_report.txt", "w", encoding="utf-8") as f:
            f.write(final_report_txt)

        # 2. HTML ë¦¬í¬íŠ¸ ìƒì„± (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ìš©)
        html_lines = [
            "<html><head><meta charset='utf-8'><style>",
            "body { font-family: 'Malgun Gothic', dotum, sans-serif; line-height: 1.6; max-width: 800px; margin: 20px auto; padding: 20px; border: 1px solid #ddd; border-radius: 10px; }",
            "h1 { color: #2c3e50; text-align: center; border-bottom: 2px solid #3498db; padding-bottom: 10px; }",
            "h3 { color: #2980b9; margin-top: 30px; border-left: 5px solid #3498db; padding-left: 10px; }",
            "ul { list-style: none; padding: 0; }",
            "li { margin-bottom: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; }",
            "a { color: #3498db; text-decoration: none; font-weight: bold; font-size: 1.1em; }",
            "a:hover { text-decoration: underline; color: #2980b9; }",
            ".info { color: #7f8c8d; font-size: 0.9em; margin-bottom: 20px; }",
            ".footer { margin-top: 40px; text-align: center; color: #95a5a6; font-size: 0.9em; border-top: 1px solid #eee; padding-top: 20px; }",
            "</style></head><body>",
            f"<h1>ğŸ“… ì˜¤ëŠ˜ì ë‰´ìŠ¤ ìˆ˜ì§‘ ë¦¬í¬íŠ¸</h1>",
            f"<div class='info'>ğŸ“… <b>ìˆ˜ì§‘ ì¼ì‹œ:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>",
            f"ğŸ” <b>í•„í„° í‚¤ì›Œë“œ:</b> {', '.join(self.money_keywords)}</div>"
        ]

        for entry in collected_data:
            html_lines.append(f"<h3>ğŸ“Œ {entry['site_name']} ({len(entry['news_list'])}ê±´)</h3><ul>")
            for news in entry['news_list']:
                html_lines.append(f"<li><a href='{news['link']}' target='_blank'>{news['title']}</a></li>")
            html_lines.append("</ul>")

        html_lines.append(f"<div class='footer'>âœ… ì´ {total_count}ê±´ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.</div>")
        html_lines.append("</body></html>")
        
        final_report_html = "".join(html_lines)
        with open("daily_news_report.html", "w", encoding="utf-8") as f:
            f.write(final_report_html)
        
        print(f"\nìˆ˜ì§‘ ì™„ë£Œ! ì´ {total_count}ê±´. TXT/HTML ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return final_report_txt


if __name__ == "__main__":
    engine = IntegratedNewsEngine()
    engine.run()
