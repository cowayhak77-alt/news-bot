import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
import random
from datetime import datetime
# ìˆ˜ì§‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì¶”ê°€/ìˆ˜ì • ê°€ëŠ¥)
TARGET_SITES = [
    {"name": "ê³ ìš©ë…¸ë™ë¶€", "url": "https://www.moel.go.kr/news/enews/report/enewsList.do"},
    {"name": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€", "url": "https://www.mss.go.kr/site/smba/ex/bbs/List.do?cbIdx=248"},
    {"name": "ì„œìš¸ì‹œì²­", "url": "https://www.seoul.go.kr/news/news_report.do"},
    {"name": "ê²½ê¸°ë„ì²­", "url": "https://www.gg.go.kr/bbs/board.do?bsIdx=469&menuId=1536"},
    {"name": "ì‚°ì—…í†µìƒìì›ë¶€", "url": "https://www.motie.go.kr/motie/ne/presse/press.jsp"}
]


class IntegratedNewsEngine:
    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        # ëˆì´ ë˜ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        self.money_keywords = ["ë³´ë„", "ìë£Œ", "ê³µê³ ", "ì§€ì›", "ì‚¬ì—…", "ëª¨ì§‘", "ì„ ì •", "ì˜ˆì‚°", "íˆ¬ì", "ìœ¡ì„±", "í˜œíƒ", "ë³´ì¡°ê¸ˆ"]
        self.table_patterns = [
            "table.board-list", "table.list_table", "table.bbs_list", 
            "table.tbl_board", "table.tstyle_list", ".board_list table",
            "table[summary*='ê²Œì‹œíŒ']", "table.table", ".board_list", ".list_type",
            ".news_list", ".bbsList", ".boardList", ".list_item"
        ]

    def get_headers(self):
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.google.com/",
            "Connection": "keep-alive"
        }

    def is_money_news(self, title):
        # í‚¤ì›Œë“œ í•„í„°ë§ ë¡œì§
        for kw in self.money_keywords:
            if kw in title:
                return True
        return False

    def smart_scrape(self, url):
        try:
            session = requests.Session()
            response = session.get(url, headers=self.get_headers(), timeout=15)
            response.raise_for_status()
            
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
                
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = []
            for pattern in self.table_patterns:
                areas = soup.select(pattern)
                for area in areas:
                    found = area.select("tbody tr, tr, li, .item, .list_item, .post-item")
                    if found: rows.extend(found)
            
            if not rows:
                content_area = soup.select_one("#contents, #content, .content, main")
                if content_area:
                    rows = content_area.select("tr, li, div[class*='item']")
                else:
                    rows = soup.select("tr, li")

            results = []
            seen_titles = set()
            
            for row in rows:
                links = row.find_all("a")
                if not links: continue
                
                valid_links = [l for l in links if len(l.get_text(strip=True)) > 5]
                if not valid_links: continue
                
                title_tag = max(valid_links, key=lambda x: len(x.get_text(strip=True)))
                title = title_tag.get_text(strip=True)
                title = re.sub(r"\[ê³µì§€\]|\[ìƒˆê¸€\]|NEW", "", title).strip()
                
                if title in seen_titles or len(title) < 5: continue
                
                # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
                if not self.is_money_news(title): continue
                
                seen_titles.add(title)
                link = urljoin(url, title_tag['href'])
                
                results.append({
                    "title": title,
                    "link": link,
                    "date": datetime.now().strftime("%Y-%m-%d") # ì‹¤ì œ ë‚ ì§œ ì¶”ì¶œ ë¡œì§ì€ ì´ì „ ì½”ë“œ ì°¸ê³ 
                })
                if len(results) >= 5: break # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 5ê±´ë§Œ ìˆ˜ì§‘
                
            return results
        except Exception as e:
            return None

    def run(self):
        report = []
        report.append(f"ğŸ“… ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ” í•„í„° í‚¤ì›Œë“œ: {', '.join(self.money_keywords)}")
        report.append("="*60)

        total_count = 0
        for site in TARGET_SITES:
            print(f"[{site['name']}] ìˆ˜ì§‘ ì¤‘...")
            news_list = self.smart_scrape(site['url'])
            if news_list:
                report.append(f"\nğŸ“Œ {site['name']} ({len(news_list)}ê±´)")
                for news in news_list:
                    report.append(f"- {news['title']}")
                    report.append(f"  ğŸ”— {news['link']}")
                total_count += len(news_list)
            time.sleep(random.uniform(0.5, 1.5))

        report.append("\n" + "="*60)
        report.append(f"âœ… ì´ {total_count}ê±´ì˜ 'ëˆ ë˜ëŠ” ì •ë³´'ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        final_report = "\n".join(report)
        
        # íŒŒì¼ë¡œ ì €ì¥ (ì´ë©”ì¼ ë°œì†¡ ëŒ€ì‹  íŒŒì¼ ì €ì¥ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í™•ì¸ ê°€ëŠ¥í•˜ê²Œ í•¨)
        with open("daily_news_report.txt", "w", encoding="utf-8") as f:
            f.write(final_report)
        
        print(f"\nìˆ˜ì§‘ ì™„ë£Œ! ì´ {total_count}ê±´. ê²°ê³¼ê°€ /home/ubuntu/daily_news_report.txt ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return final_report

if __name__ == "__main__":
    engine = IntegratedNewsEngine()
    engine.run()
